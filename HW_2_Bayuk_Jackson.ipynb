# -*- coding: utf-8 -*-
"""
Created on Tue Feb 18 18:04:05 2025

@author: Jackson Bayuk
"""
#AI (chatgpt) Prompts I used: How do I change my working directory in python?
#How do I split a data into triaing and test data?
#How do I create a SVC model in python?
#What is the difference between a SVC model and a fastText model?


#Question One
#First I have to import the Data set (I had to update my working directory first)
import pandas as pd
import os
os.chdir("C:\\Users\\Jackson Bayuk\\Downloads")
df = pd.read_csv("C:\\Users\\Jackson Bayuk\\Downloads\\data.csv")

print(df.head())

#Question Two
#I had to import the packages needed for this assignment. 
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re

#Here I am splitting the data into train and test data, using the test size of 0.2.
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Sentiment'])

#Here I set the range for extracting unigrams or single words as 1, so that it 
#would simply select individual words as features. In addition, I removed all
#of the stop words such as 'the' and 'is'. I removed stop words becuase they words
#won't add much meaing to a sentence. I also saw your note from last assignment 
#telling me that I should have removed the stop words in question 4. In addition, 
#after removing numbers, which most likely will not help predict the sentiment, 
#the accuracy went up by about 1.5%.
def remove_stopwords_and_numbers(text):
    # Remove numbers
    text = re.sub(r'\d+', '', text)  # Remove digits
    # Here, 'stop_words' will be handled by CountVectorizer itself
    return text
vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', preprocessor=remove_stopwords_and_numbers) 
#Here I get the sentences into a sparse matrix and then the rest of the words are 
#transformed based on the vocabulary learned from training the data.
X_train = vectorizer.fit_transform(train_data['Sentence'])
X_test = vectorizer.transform(test_data['Sentence'])

#Here I am converting the three sentiments into numerical values. 
sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}
y_train = train_data['Sentiment'].map(sentiment_mapping)
y_test = test_data['Sentiment'].map(sentiment_mapping)


#Question Three
#Now I am training the SVC model.
classifier = LinearSVC(random_state=42, max_iter=5000)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)


#Here get the accuracy of my model and the f1-score for negative, neutral and
#positive sentiments.
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))


#Here I am completing feature analysis, getting feature importance and producing
#the top 20 import positive and negative features. 
feature_names = np.array(vectorizer.get_feature_names_out())
coefficients = classifier.coef_[0]

top_positive_indices = np.argsort(coefficients)[-20:]
top_negative_indices = np.argsort(coefficients)[:20]

#These are the top 20 Positive Features
print(feature_names[top_positive_indices])

#These are the top 20 negative Features
print(feature_names[top_negative_indices])

conf_matrix = confusion_matrix(y_test, y_pred, labels=[-1, 0, 1])  

fp_indices = np.where(y_pred > y_test)[0]  # False Positives
fn_indices = np.where(y_pred < y_test)[0]  # False Negatives

#These are the 25 false positives
for i in fp_indices[:25]:
    print(f"Sentence: {test_data.iloc[i]['Sentence']} | True: {test_data.iloc[i]['Sentiment']} | Predicted: {list(sentiment_mapping.keys())[list(sentiment_mapping.values()).index(y_pred[i])]}")
#Some liguistic patterns that I'm noticing when looking through these false 
#positives are that much of the time when the sentiment is actually negative, but
#is predicted as something else there are numbers involved, so the sentance will
#say that one stock is down from one number to another number, but the model can 
#not read that since it is looking for english words to determine sentiment and 
#numbers would not give sentiment unless you know that a lesser number is negative. 


#These are the 25 false negatives
for i in fn_indices[:25]:
    print(f"Sentence: {test_data.iloc[i]['Sentence']} | True: {test_data.iloc[i]['Sentiment']} | Predicted: {list(sentiment_mapping.keys())[list(sentiment_mapping.values()).index(y_pred[i])]}")
#Some liguistic patterns that I'm noticing when looking through these false negatives
#are that when the sentiment is actually positive or netutral, but is predicted 
#as something else there are once again numbers involved (such as numbers going 
#up), which would be positive. In addition, there are some sentences where someone
#gave a warning to someone else and a warning could be taken as negative, but 
#becuase the model doesn't have the context it puts it as negative when it may 
#actually have been a helpfull warning. These are some of the things I'm noticing
#looking at the false positive and negatives. 

#Question 4
import gensim.downloader as api

#Load in the model
fasttext_model = api.load("fasttext-wiki-news-subwords-300") 

#Create a function to get the sentences into vectors, seperating each word. 
def get_sentence_vector(Sentence, model):
    words = Sentence.split()  # Tokenize sentence
    word_vectors = [model[word] for word in words if word in model]  
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)  
    return np.mean(word_vectors, axis=0)

df['vector'] = df['Sentence'].apply(lambda x: get_sentence_vector(str(x), fasttext_model))

# Stack vectors into feature matrix
X = np.vstack(df['vector'].values)

sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}
y = df['Sentiment'].map(sentiment_mapping)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Here I train the Logistic Regression classifier model
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(max_iter=1000, random_state=42)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

#Accuracy and F1-Score
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)
print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))

fp_indices = np.where(y_pred > y_test)[0]  # False Positives
fn_indices = np.where(y_pred < y_test)[0]  # False Negatives

#These are the false positives and false negatives with the new model

for i in fp_indices[:25]:
    print(f"Sentence: {df.iloc[i]['Sentence']} | True: {df.iloc[i]['Sentiment']} | Predicted: {list(sentiment_mapping.keys())[list(sentiment_mapping.values()).index(y_pred[i])]}")

for i in fn_indices[:25]:
    print(f"Sentence: {df.iloc[i]['Sentence']} | True: {df.iloc[i]['Sentiment']} | Predicted: {list(sentiment_mapping.keys())[list(sentiment_mapping.values()).index(y_pred[i])]}")

#The results for the fasttext_model is that it has a slightly higher accuracy
#than the SCV model (close to about a 65% accuracy). In addition, the false positives 
#are mostly neutral that are predicted positive and not negative predicted as positive.
#The problems that this model has that is similar to the other model is that it has
#difficulties when numbers are involved and understanding the context. 


#Question 5
#Both of these models, the SVC model and the fastText model do a fine job at 
#analyzing financial sentiment data. However they both have their own strengths
#and weaknesses. For the SCV model it is good at analyzing high dimensional data,
#which financial data is. It's weaknesses are that it is not so good with 
#multi-class data which we have here where there are three options such as 
#positive, negative and neutral, which this data has. For the fastText model, it
#may be more effective (which was seen by its increase in accuracy when compared 
#to the other model) becuase it has the ability to capture semantic relationships, 
#meaning it can notice the relationship between the words stock and shares, which 
#is helpful in this case becuase context is key. The bad part is fastText models are 
#prone to noisy predictions in complex datasets. Ultimately, both of these models 
#did a fine job at analyzing financial sentiment analysis, however where they struggle
#is when numbers are involved, complicating the data and not being able to 
#completing understand the context around the sentances. 
